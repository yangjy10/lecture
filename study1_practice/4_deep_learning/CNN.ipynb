{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convolutional Nueral Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1) import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2) define placeholder for INPUT & LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = tf.placeholder(tf.float32, [None, 28*28])\n",
    "LABELS = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2차원 매트릭스로 만든다?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) define cnn model\n",
    "\n",
    "<img src=\"./images/cnn.png\" alt=\"slp model\" width=1000 align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction = convolutional_neural_network(input=IMAGES, output_dim=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  - define convolutional_neural_network function with tf.nn.conv2d, tf.nn.max_pool, tf.nn.tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_neural_network(input, output_dim=None):\n",
    "    image = tf.reshape(input, [-1, 28, 28, 1]) #batch_size x width x height x channel(흑백 채널 1개) # -1 값 None 같은 역할\n",
    "\n",
    "    # Conv layer1\n",
    "    # Filter가 Weight 역활을 함\n",
    "    W_conv1 = tf.Variable(tf.truncated_normal([5, 5, 1, 20], stddev=0.1))\n",
    "    b_conv1 = tf.Variable(tf.zeros([20])) # 바이어스 차원도 20개\n",
    "    h_conv1 = tf.nn.conv2d(\n",
    "            image, \n",
    "            W_conv1, # 필터가 웨이트 역할\n",
    "            strides=[1, 1, 1, 1], \n",
    "            padding='SAME') + b_conv1 # batch_sizex28x28x20, 이미지 차원 유지되게 패딩 SAME\n",
    "    fmap_conv1 = tf.nn.tanh(h_conv1)\n",
    "\n",
    "    # Pooling(Max) layer1\n",
    "    # k_size = [one_image, width, hegiht, one_channel]\n",
    "    h_pool1 = tf.nn.max_pool(\n",
    "        fmap_conv1, \n",
    "        ksize=[1, 2, 2, 1], \n",
    "        strides=[1, 2, 2, 1], \n",
    "        padding='SAME'\n",
    "    ) # batch_sizex14x14x20 stride 2개 28 -> 14로 줄어듬\n",
    "\n",
    "    # Conv layer2\n",
    "    # Filter가 Weight 역활을 함\n",
    "    W_conv2 = tf.Variable(tf.truncated_normal([5, 5, 20, 50], stddev=0.1))\n",
    "    b_conv2 = tf.Variable(tf.zeros([50]))\n",
    "    h_conv2 = tf.nn.conv2d(\n",
    "        h_pool1, \n",
    "        W_conv2, \n",
    "        strides=[1, 1, 1, 1], \n",
    "        padding='SAME') + b_conv2 # batch_sizex14x14x50, 50번 적용 \n",
    "    fmap_conv2 = tf.nn.tanh(h_conv2)\n",
    "\n",
    "    # Pooling(Max) layer2\n",
    "    h_pool2 = tf.nn.max_pool(\n",
    "        fmap_conv2, \n",
    "        ksize=[1, 2, 2, 1], \n",
    "        strides=[1, 2, 2, 1], \n",
    "        padding='SAME'\n",
    "    ) # batch_sizex7x7x50\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 50 * 7 * 7])  # batch_sizex(7x7x50), 평평하게 펴줘야 함\n",
    "    \n",
    "    # fully-connected layer1 \n",
    "    W_fc1 = tf.Variable(tf.truncated_normal([50 * 7 * 7, 500], stddev=0.1))\n",
    "    b_fc1 = tf.Variable(tf.zeros([500]))\n",
    "    h_fc1 = tf.nn.tanh(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) # batch_sizex500 # 차원 줄여줌\n",
    "\n",
    "    # fully-connected layer2\n",
    "    W_fc2 = tf.Variable(tf.truncated_normal([500, output_dim], stddev=0.1))\n",
    "    b_fc2 = tf.Variable(tf.zeros([output_dim]))\n",
    "    output = tf.matmul(h_fc1, W_fc2) + b_fc2 #batch_sizex10\n",
    "\n",
    "    return output\n",
    "\n",
    "prediction = convolutional_neural_network(INPUT, output_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=LABELS, logits=prediction\n",
    ")\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-f659c5e1ce47>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/ray/multicamp/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/ray/multicamp/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ray/multicamp/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ray/multicamp/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ray/multicamp/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"./data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) start training\n",
    "   #### - set training parameters : batch size, learning rate, total loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.01\n",
    "TOTAL_LOOP = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - arrA = [[0,0,0,0,1],[0,1,0,0,0]]\n",
    " - np.where(arrA) => ([0,1], [4,1])\n",
    " - ref) https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.where.html?highlight=numpy%20where#numpy.where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop: 00500, loss: 0.24762183\n",
      "loop: 01000, loss: 0.19355531\n",
      "loop: 01500, loss: 0.15968311\n",
      "loop: 02000, loss: 0.13976234\n",
      "loop: 02500, loss: 0.06908678\n",
      "loop: 03000, loss: 0.09957447\n",
      "loop: 03500, loss: 0.10375062\n",
      "loop: 04000, loss: 0.05489205\n",
      "loop: 04500, loss: 0.120919704\n",
      "loop: 05000, loss: 0.078065485\n",
      "loop: 05500, loss: 0.018431403\n",
      "loop: 06000, loss: 0.03395952\n",
      "loop: 06500, loss: 0.082293175\n",
      "loop: 07000, loss: 0.05262528\n",
      "loop: 07500, loss: 0.10878975\n",
      "loop: 08000, loss: 0.071632974\n",
      "loop: 08500, loss: 0.06618172\n",
      "loop: 09000, loss: 0.07681943\n",
      "loop: 09500, loss: 0.012724737\n",
      "loop: 10000, loss: 0.023088682\n",
      "Training Finished! (loss : 0.023088682)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for loop in range(1, TOTAL_LOOP + 1):\n",
    "    \n",
    "    train_images, train_labels = mnist.train \\\n",
    "                                      .next_batch(BATCH_SIZE)\n",
    "    train_labels = np.where(train_labels)[1]\n",
    "        \n",
    "    _, loss = sess.run(\n",
    "        [optimizer, cost],\n",
    "        feed_dict={\n",
    "            INPUT: train_images, \n",
    "            LABELS: train_labels\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if loop % 500 == 0 or loop == 0:\n",
    "        print(\"loop: %05d,\"%(loop), \"loss:\", loss)\n",
    "\n",
    "print(\"Training Finished! (loss : \" + str(loss) + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) test performance\n",
    "\n",
    " - test image shape: (100, 784)\n",
    " - test label shape: (100, 10) \n",
    " \n",
    "\n",
    " - arrB = [[0, 1, 2],[3, 4, 5]]\n",
    " - np.argmax(arrB) => 5\n",
    " - np.argmax(arrB, axis=0) => [1, 1, 1]\n",
    " - np.argmax(arrB, axis=1) => [2, 2]\n",
    " - ref) https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.argmax.html\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: {1: 95, 2: 100, 3: 99, 4: 99, 5: 99}\n",
      "Total mean Accuracy: 98.4\n"
     ]
    }
   ],
   "source": [
    "TEST_SAMPLE_SIZE = 100\n",
    "TEST_NUMBER = 5\n",
    "accuracy_save = dict()\n",
    "\n",
    "for number in range(1, 1+TEST_NUMBER):\n",
    "    \n",
    "    test_images, test_labels = mnist.test \\\n",
    "                                    .next_batch(TEST_SAMPLE_SIZE)\n",
    "    pred_result = sess.run(\n",
    "        prediction, \n",
    "        feed_dict={INPUT: test_images}\n",
    "    )\n",
    "\n",
    "    pred_number = np.argmax(pred_result, axis=1) # 100x1\n",
    "    label_number = np.where(test_labels)[1] #100x1\n",
    "    \n",
    "    accuracy_save[number] = np.sum(pred_number == label_number)\n",
    "    \n",
    "print(\"Accuracy:\", accuracy_save)\n",
    "print(\"Total mean Accuracy:\", \n",
    "      np.mean(list(accuracy_save.values()))\n",
    ")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
